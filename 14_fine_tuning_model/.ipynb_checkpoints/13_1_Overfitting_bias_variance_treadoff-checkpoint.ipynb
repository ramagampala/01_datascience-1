{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under fitting vs Over Fitting \n",
    "  * **Over Fitting : ** The model performs well on training data, but failes to generalize.\n",
    "  <p>&nbsp;&nbsp;&nbsp;&nbsp;* Overfitting happens when the model is too complex relative to the amount of noisiness in the training data. The possible solutions are:</p>\n",
    "  <p>&nbsp;&nbsp;&nbsp;&nbsp;* I) Simplify the model by selecting one with fewer parameters (e.g., a linear model rather than a high-degree ploynomial model), by reducing the number of attributes in the training data or by constraining the model (constraining a model to make it simpler and reduce the risk of overfitting is called **regularization** - which is controlled by a hyperparameter of a learning algorithm).</p>\n",
    "  <p>&nbsp;&nbsp;&nbsp;&nbsp;* II) Gather more training data</p>\n",
    "  <p>&nbsp;&nbsp;&nbsp;&nbsp;* III) Reduce the noise in training data (e.g., fix data errors and remove outliers).</p>\n",
    "  * ** Unberfitting : ** The model is too simple to learn the underlying structure of the data. To fix this.\n",
    "  <p>&nbsp;&nbsp;&nbsp;&nbsp; I) Select a more powerful model, with more parameters.</p>\n",
    "  <p>&nbsp;&nbsp;&nbsp;&nbsp; II) Feeding better features to the learning algorithm (feature engineering)</p>\n",
    "  <p>&nbsp;&nbsp;&nbsp;&nbsp; III) Reducing the constrains on the model (e.g., reducing the regularization hyperparameter.) </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Tread-Off \n",
    "  * **Bias : ** This part of the generalization error is due to wrong assumptions, such as assuming that the data is linear when it is acctually quadratic. A high Bias model is most likely to **underfit TRAIN dataset**. \n",
    "  * **Variance : ** This part is due to excessive sensitivity to small variations in the training data. A Model with many digrees of freedom is likely to have high variance, and thus to overfit the training data.\n",
    "  * **Irreducible Error : ** This part is due to noise in the data. Only way to handle this by cleaning the data (e.g., fix the data source, such as broken sensors, delete outliers.)\n",
    "<p>Increasing a model's complexity will typically increase its variance and reduce bias. Conversly, reduce a model's complexity increases its bias and reduces its variance. This is why it is called as Tread-Off</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Challenges of Machine Learning\n",
    "  * In short we may have to train ML algorithom on some data. The two things that may go wrong or 'Bad Algorithom' or 'Bad Data'.\n",
    "  * **Insufficient Quantity or Train Data : ** Most Machine Learning algorithm takes a lot of data to work properly. Simple problems it may need thousnads of examples, for complex porblems like image processing and speech recognition millions of example are need (unless you can use parts of existing models - Deep Learning).\n",
    "  * **Nonrepresentative Training Data : ** Inorder to generalize well, it is very important that your training data be representative of the new cases you want to generalize to. This is true whether you are using model-based or instance-based learning.\n",
    "  <p>This is harder than it sounds, if the sample size is too small you will have sampling noice (i.e. nonrepresentative data as a result of chanse). Very large samples can be nonrepresentative if the sampling method is flawed (called as sampling bias). </p>\n",
    "  <p> **Example : ** US presidential elections 1930, which pitted Landon against Roosevelt: The **Literary Digest** conducted a very large poll, sending mail to about 10 million people. It got 2.4 million answers, and predicted with high confidence that Landon would get 57% of the votes. Instead, Roosevelt won with 62% of the votes. The flaw was in **Literary Digest's** sampling method:</p>\n",
    "  <p>\n",
    "  \n",
    "  <p>* &nbsp;&nbsp;&nbsp;&nbsp; I) Address obtaining process : Literary Digest used telephone directories, lists of meagazine subscribers, club membership lists, and the like. **All of these lists tend to favor wealthier people, who are more likely to vote Republican(hence Landon).**</p>\n",
    "  <p> &nbsp;&nbsp;&nbsp;&nbsp; II) Less than 25% of the people who received the poll answered. Again, this introduces a sampling bias, by ruling out people who don't care much about politics, people who don't like the Literary Digest, and other key groups. This is a special type of sampling bias called nonreponse bias.</p>\n",
    "  \n",
    "  * ** Poor-Quality Data : ** If the training data is full of errors, outliers, and noise (e.g., due to poor-quality measurements), it will make it harder for the system to detect the underlying patterns, so the system is less likely to perform well. The truth is, most data scientists spend a significant part of their time in cleaning the data. (e.g., Discard outliers or fix the error manually. Ignore rows with missing data or impute).\n",
    "  \n",
    "  * **Irrelevent Features : ** System will be capable of learning if the training data contains enough relevant features and not too many irrelevent ones. A critical part of the suceess of a Machine Learning project is coming up with a good set of features to train on. This process is called **feature engineeting**, this involves.\n",
    "  <p>&nbsp;&nbsp;&nbsp;&nbsp;I) **Feature Selection : ** selecting the most useful features to train on.</p>\n",
    "  <p>&nbsp;&nbsp;&nbsp;&nbsp;II) **Feature Extraction : ** combine existing features to produce a more useful one(dimentionality reduction can help).</p>\n",
    "  <p>&nbsp;&nbsp;&nbsp;&nbsp;III) **Create new features** by gathering new data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Click <a href=\"./OverfittingNotes/Over fitting - Regularization.html\"> Here</a> for more on this topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
