{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "  * Decision trees will have \n",
    "    * **Notes :** Test for the value of a certain attribute\n",
    "    * **Edges :** Connects to next note or leaf\n",
    "    * **Leaves :** Terminal node that predicts the outcome\n",
    "    \n",
    "    <img height=\"600\" width=\"750\" src=\"sample_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree algorithm is knows as divide & conqure algorithm\n",
    "  * step 1 : split into subsets\n",
    "  * step 2 : check if the subset is pure or not (all yes or all no) ?\n",
    "  * step 3 : if all \"yes\" or \"no\" in a subset **stop**\n",
    "  * step 4 : if not **repear** step1 to 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset\n",
    "<img height=\"500\" width=\"650\" src=\"sample_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does a tree decide where to split ?, that is which column is best to consider as root node or further sub-nodes ? . Lets take sample data given above.\n",
    "\n",
    "  * We have different attribute selection methods (chekc purity of a subset). Based on which we are going to decide the split.\n",
    "    * For Classification Data Sets\n",
    "      * Gini Index\n",
    "      * Information Gain\n",
    "      * Gain Ratio\n",
    "    * For Regression Data Sets\n",
    "      * Reduction in Variance\n",
    "      \n",
    "### Lets understand all these approaches.\n",
    "####  Gini Index (http://dni-institute.in/blogs/cart-decision-tree-gini-index-explained) :  Gini Index is one of the impurity measure. Lesser the value more significant the feature.\n",
    "\n",
    "  * GINI Index = $ \\sum_{i != j } P(i)P(j) $  = $1 - \\sum_{t=1}^k P_t^2  $ **, Maximum value of Gini Index ** could be when all target values are equally distributed.\n",
    "  * For Binary Target variable, **Maximum Gini Index** value is = $ 1 - (1/2)^2 - (1/2)^2 $ = 0.5\n",
    "  * Similarly for Nominal variable with k level, the **Maximum Gini Index** is = 1 - 1/k\n",
    "  * Minimum value of Gini Index will be 0, this happens when all observations belong to one label\n",
    "  * Gini Index for an Attribute Gini(A) = $\\sum_{v} P(v) \\sum_{i!=j} P(i/v)P(j/v) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets apply Gini-Index (ID3 - Algorithm) and calulate gain to decide the root-nodes for above data.\n",
    "\n",
    "  * Gain(PlayTennis) = $ 1 - P(\"YES\")^2 - P(\"NO\")^2 $\n",
    "                      = 1 - (9/14)^2 - (5/14)^2\n",
    "                      = 1 - 0.4132 - 0.1275\n",
    "                      = 0.4593\n",
    "  * Gain(Outlook) = P(sunny) P(YES/Sunny) P(NO/Sunny) + P(Overcast) P(YES/Overcast) P(NO/Overcast) + P(Rain) P(YES/Rain) P(NO/Rain)                      \n",
    "                   = 5/14 * 2/5 * 3/5 + 4/14 * 4/4 * 0/4 + 5/14 * 3/5 * 2/5   \n",
    "                   = 30/350 + 0 + 30/350\n",
    "                   = 0.1714\n",
    "  * Gain(Temperature) = P(Hot) P(YES/Hot) P(NO/Hot) + P(Mild) P(YES/Mild) P(NO/Mild) + P(Cold) P(YES/Cold) P(NO/Cold)                      \n",
    "                   = 4/14 * 2/4 * 2/4 + 6/14 * 4/6 * 2/6 + 4/14 * 3/4 * 1/4   \n",
    "                   = 16/224 + 48/504 + 12/224\n",
    "                   = 0.0714 + 0.0952 + 0.0535\n",
    "                   = 0.2201\n",
    "  * Gain(Humidity) = P(High) P(YES/High) P(NO/High) + P(Normal) P(YES/Normal) P(NO/Normal)                     \n",
    "                   = 7/14 * 3/7 * 4/7 + 7/14 * 6/7 * 1/7   \n",
    "                   = 12/98 + 6/98\n",
    "                   = 18/98\n",
    "                   = 0.1836\n",
    "  * Gain(Wind) = P(Weak) P(YES/Weak) P(NO/Weak) + P(Strong) P(YES/Strong) P(NO/Strong)                     \n",
    "                   = 7/14 * 5/7 * 2/7 + 7/14 * 3/7 * 4/7   \n",
    "                   = 10/98 + 12/98\n",
    "                   = 22/98\n",
    "                   = 0.2244        \n",
    "### Now calculate Gini Gain for each attribute.\n",
    "  * GiniGain(Outlook) = Gain(PlayTennis) - Gain(Overlook) \n",
    "                       = 0.4593 - 0.1714 \n",
    "                       = 0.2879\n",
    "  * GiniGain(Temperature) = Gain(PlayTennis) - Gain(Temperature) \n",
    "                       = 0.4593 - 0.2201\n",
    "                       = 0.2392\n",
    "  * GiniGain(Humidity) = Gain(PlayTennis) - Gain(Humidity) \n",
    "                       = 0.4593 - 0.1836\n",
    "                       = 0.2757\n",
    "  * GiniGain(Wind) = Gain(PlayTennis) - Gain(Wind) \n",
    "                       = 0.4593 - 0.2244\n",
    "                       = 0.2349                      \n",
    "### The Gini Gain for \"Outlook\" attribute is more, hence we select Outlook as root node.\n",
    "\n",
    "<img height=\"700\" width=\"900\" src=\"first_split.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Second level split for Outlook = \"Sunny\" (which feature to split on?)\n",
    "\n",
    "<img height=\"400\" width=\"500\" src=\"outlook_sunny.png\">\n",
    "\n",
    "  * Gain(PlayTennis) = $ 1 - P(\"YES\")^2 - P(\"NO\")^2 $\n",
    "                      = 1 - (2/5)^2 - (3/5)^2\n",
    "                      = 1 - 0.16 - 0.36\n",
    "                      = 0.48\n",
    "                      \n",
    "  * Gain(Temperature) = P(Hot) P(YES/Hot) P(NO/Hot) + P(Mild) P(YES/Mild) P(NO/Mild) + P(Cold) P(YES/Cold) P(NO/Cold)                      \n",
    "                   = 2/5 * 0/2 * 2/2 + 2/5 * 1/2 * 1/2 + 1/5 * 1/1 * 0/1   \n",
    "                   = 0 + 1/10 + 0\n",
    "                   = 0.1\n",
    "  * Gain(Humidity) = P(High) P(YES/High) P(NO/High) + P(Normal) P(YES/Normal) P(NO/Normal)                     \n",
    "                   = 3/5 * 0/3 * 3/3 + 2/5 * 2/2 * 0/2   \n",
    "                   = 0\n",
    "  * Gain(Wind) = P(Weak) P(YES/Weak) P(NO/Weak) + P(Strong) P(YES/Strong) P(NO/Strong)                     \n",
    "                   = 3/5 * 1/3 * 2/3 + 2/5 * 1/2 * 1/2  \n",
    "                   = 2/15 + 1/10\n",
    "                   = 0.2333\n",
    "### Gini Gain for each attribute\n",
    "  * GiniGain(Temperature) =  Gain(PlayTennis) - Gain(Temperature)\n",
    "                          = 0.48 - 0.1\n",
    "                          = 0.38\n",
    "  * GiniGain(Humidity) =  Gain(PlayTennis) - Gain(Humidity)\n",
    "                          = 0.48 - 0\n",
    "                          = 0.48          \n",
    "  * GiniGain(Wind) =  Gain(PlayTennis) - Gain(Wind)\n",
    "                          = 0.48 - 0.2333\n",
    "                          = 0.2467   \n",
    "### Gini Gain for \"Humidity\" is more, hence the split for Outlook=\"Sunny\" node is on \"Humidity\" \n",
    "<img height=\"700\" width=\"900\" src=\"second_1_split.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second level split for Outlook = \"Overcast\". No need to split as \"PlayTenis\" = YES for all records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second level split for Outlook = \"Rain\" (which feature to split on?)\n",
    "\n",
    "  * Gain(PlayTennis) = $ 1 - P(\"YES\")^2 - P(\"NO\")^2 $\n",
    "                      = 1 - (3/5)^2 - (2/5)^2\n",
    "                      = 1 - 0.36 - 0.16 \n",
    "                      = 0.48\n",
    "                      \n",
    "  * Gain(Temperature) = P(Mild) P(YES/Mild) P(NO/Mild) + P(Cold) P(YES/Cold) P(NO/Cold)                      \n",
    "                   = 3/5 * 2/3 * 1/3 + 2/5 * 1/2 * 1/2   \n",
    "                   = 2/15 + 1/10\n",
    "                   = 0.2333\n",
    "  * Gain(Humidity) = P(High) P(YES/High) P(NO/High) + P(Normal) P(YES/Normal) P(NO/Normal)                     \n",
    "                   = 2/5 * 1/2 * 1/2 + 3/5 * 2/3 * 1/3   \n",
    "                   = 1/10 + 2/15\n",
    "                   = 0.2333\n",
    "  * Gain(Wind) = P(Weak) P(YES/Weak) P(NO/Weak) + P(Strong) P(YES/Strong) P(NO/Strong)                     \n",
    "                   = 2/5 * 2/2 * 0/2 + 3/5 * 1/3 * 2/3  \n",
    "                   = 0 + 2/15\n",
    "                   = 0.1333\n",
    "### Gini Gain for each attribute\n",
    "  * GiniGain(Temperature) =  Gain(PlayTennis) - Gain(Temperature)\n",
    "                          = 0.48 - 0.2333\n",
    "                          = 0.2467 \n",
    "  * GiniGain(Humidity) =  Gain(PlayTennis) - Gain(Humidity)\n",
    "                          = 0.48 - 0.2333\n",
    "                          = 0.2467           \n",
    "  * GiniGain(Wind) =  Gain(PlayTennis) - Gain(Wind)\n",
    "                          = 0.48 - 0.1333\n",
    "                          = 0.3467   \n",
    "### Gini Gain for \"Wind\" is more, hence the split when overcast=\"Rain\" is on \"Wind\" \n",
    "<img height=\"700\" width=\"900\" src=\"second_2_split.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Node needed to be splitted is Outlook - \"Rain\" -> Wind - \"Strong\" one. As this Node has records of two classes.\n",
    "\n",
    "### Third level split for Outlook - \"Rain\" -> Wind - \"Strong\" (which feature to split on?)\n",
    "\n",
    "  * Gain(PlayTennis) = $ 1 - P(\"YES\")^2 - P(\"NO\")^2 $\n",
    "                      = 1 - (1/3)^2 - (2/3)^2\n",
    "                      = 1 - 0.1111 - 0.4444\n",
    "                      = 0.4445\n",
    "                      \n",
    "  * Gain(Temperature) = P(Mild) P(YES/Mild) P(NO/Mild) + P(Cold) P(YES/Cold) P(NO/Cold)                      \n",
    "                   = 2/3 * 1/2 * 1/2 + 1/3 * 0/2 * 1/1   \n",
    "                   = 1/6 + 0\n",
    "                   = 0.1666\n",
    "  * Gain(Humidity) = P(High) P(YES/High) P(NO/High) + P(Normal) P(YES/Normal) P(NO/Normal)                     \n",
    "                   = 1/3 * 0/1 * 1/1 + 2/3 * 1/2 * 1/2   \n",
    "                   = 0 + 1/6\n",
    "                   = 0.1666\n",
    "\n",
    "### Gini Gain for each attribute\n",
    "  * GiniGain(Temperature) =  Gain(PlayTennis) - Gain(Temperature)\n",
    "                          = 0.4445 - 0.1666\n",
    "                          = 0.2779 \n",
    "  * GiniGain(Humidity) =  Gain(PlayTennis) - Gain(Humidity)\n",
    "                          = 0.4445 - 0.1666\n",
    "                          = 0.2779        \n",
    "### Gini Gain for \"Temperature\" and \"Humidity\"  are equal, hence we can split on any.  I chose \"Temperature\"\n",
    "<img height=\"700\" width=\"900\" src=\"third_1_split.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth split is intutive, we can caluluate GiniGain if we want. The final Tree is \n",
    "\n",
    "<img height=\"800\" width=\"900\" src=\"fourth_1_split.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We Build a Tree using GiniGain, How does Tree classification work?. It works by traversing through the nodes.\n",
    "  * Let us look at below example.\n",
    "  <img height=\"100\" width=\"400\" src=\"tree_classification_data.png\">\n",
    "  <img height=\"800\" width=\"900\" src=\"tree_classification.png\">\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain \n",
    "\n",
    "  * Need to calculate Entropy H(Y) = $ - \\sum_{i=1}^K P_i log_2 P_i $ , K - number of classes in Y. This will measure \"uncertainity\" or \"impurity\" of the set.\n",
    "  * For the above dataset Entropy H(PlayTennis) = - p(\"YES\") log P(\"YES\") - P(\"NO\") log P(\"NO\")\n",
    "                                       = - 9/14 log(9/14) - 5/14 log(5/14)\n",
    "                                       = -9/14 (-0.6374) - 5/14 (-1.4854)\n",
    "                                       = 0.4098 + 0.5305\n",
    "                                       = 0.9403\n",
    "  * Condition entropy (Expected entropy of Y knowing the values of X) - <img height=\"100\" width=\"400\"  src=\"conditional_entropy.png\">\n",
    "  \n",
    "  * Information Gain (Reduction of uncertainty)   G(Y/X) = H(Y) - H(Y/X)\n",
    "\n",
    "  \n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * ConditionalEntropy(PlayTennis/Outlook) = \n",
    "                                    - P(\"Outlook-Sunny\") [P(\"YES\"/\"Outlook-Sunny\") log P(\"YES\"/\"Outlook-Sunny\") +   \n",
    "                                                           P(\"NO\"/\"Outlook-Sunny\") log P(\"NO\"/\"Outlook-Sunny\")] -\n",
    "                                    - P(\"Outlook-Overcast\") [P(\"YES\"/\"Outlook-Overcast\") log P(\"YES\"/\"Outlook-Overcast\") +\n",
    "                                                           P(\"NO\"/\"Outlook-Overcast\") log P(\"NO\"/\"Outlook-Overcast\")] -\n",
    "                                    - P(\"Overlook-Rain\") [P(\"YES\"/\"Outlook-Rain\") log P(\"YES\"/\"Outlook-Rain\") +\n",
    "                                                           P(\"NO\"/\"Outlook-Rain\") log P(\"NO\"/\"Outlook-Rain\")]                                                           \n",
    "                                  = - 5/14 [2/5 log 2/5 + 3/5 log 3/5]  \n",
    "                                    - 4/14 [4/4 log 4/4 + 0/4 log 0/4] \n",
    "                                    - 5/14 [3/5 log 3/5 + 2/5 log 2/5]\n",
    "                                  = - 0.3571 [0.4 * -1.3219 + 0.6 * -0.7369]\n",
    "                                    - 0.2857 [1 * 0 + 0 * unknown]\n",
    "                                    - 0.3571 [0.6 * -0.7369 + 0.4 * -1.3219]\n",
    "                                  = - 0.3571 [ - 0.5288 - 0.4421 ]\n",
    "                                    - 0.2857 [0 + 0]\n",
    "                                    - 0.3571 [- 0.4421 - 0.5288]  \n",
    "                                  = 2 * 0.3571 * 0.9709\n",
    "                                  = 0.6934\n",
    "                                  \n",
    "  * ConditionalEntropy(PlayTennis/Temperature) = \n",
    "                                    - P(\"Temperature-Hot\") [P(\"YES\"/\"Temperature-Hot\") log P(\"YES\"/\"Temperature-Hot\") +   \n",
    "                                                           P(\"NO\"/\"Temperature-Hot\") log P(\"NO\"/\"Temperature-Hot\")] -\n",
    "                                    - P(\"Temperature-Cool\") [P(\"YES\"/\"Temperature-Cool\") log P(\"YES\"/\"Temperature-Cool\") +\n",
    "                                                           P(\"NO\"/\"Temperature-Cool\") log P(\"NO\"/\"Temperature-Cool\")] -\n",
    "                                    - P(\"Temperature-Mild\") [P(\"YES\"/\"Temperature-Mild\") log P(\"YES\"/\"Temperature-Mild\") +\n",
    "                                                           P(\"NO\"/\"Temperature-Mild\") log P(\"NO\"/\"Temperature-Mild\")]                                    \n",
    "                                  = - 4/14 [2/4 log 2/4 + 2/4 log 2/4]\n",
    "                                    - 6/14 [4/6 log 4/6 + 2/6 log 2/6]\n",
    "                                    - 4/14 [3/4 log 3/4 + 1/4 log 1/4]\n",
    "                                  = - 0.2857 [-0.5 - 0.5]\n",
    "                                    - 0.4286 [0.6667 * -0.5848 + 0.3333 * -1.5851]\n",
    "                                    - 0.2857 [0.75 * -0.4150 +  0.25 * -2]\n",
    "                                  = 0.2857 +  0.4286 [0.3899 + 0.5283] + 0.2857 [0.3112 + 0.5]\n",
    "                                  = 0.2857 + 0.3935 + 0.2317\n",
    "                                  = 0.9109\n",
    "                                  \n",
    "  * Like wise we have to calculate ConditionalEntropy(PlayTennis/Humidity), ConditionalEntropy(PlayTennis/Wind).\n",
    "### Once after calculating Conditional Entropies, calculate Information Gain.\n",
    "  * InformationGain(PlayTennis/Outlook) = H(PlayTennis) - ConditionalEntropy(PlayTennis/Outlook)\n",
    "                                        = 0.9403 - 0.6934 = 0.2469\n",
    "  * InformationGain(PlayTennis/Outlook) = H(PlayTennis) - ConditionalEntropy(PlayTennis/Outlook)\n",
    "                                        = 0.9403 - 0.9109 = 0.0294\n",
    "  *  ...\n",
    "  *  ...\n",
    "  * The feature with highest information gain going to be the splitting feature.\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with \"Gini Gain\" (same problem will happen for \"Infomation Gain\" also).\n",
    "  * Suppose that we have taken \"Day\" attribute for spliting. This is going to generate a tree with singleton nodes, which are purest. If we go by purity measure, split on \"Day\" column is going to be best. This is not proper as any day is not going to repeate. The model will fail for unseen dataset.\n",
    "  \n",
    "  * Gini(Day) = P(d1) P(YES/d1) P(NO/d1) + P(d2) P(YES/d2) P(NO/d2) + P(d3) P(YES/d3) P(NO/d3) + P(d4) P(YES/d4) P(NO/d4) +\n",
    "                 P(d5) P(YES/d5) P(NO/d5) + P(d1) P(YES/d6) P(NO/d6) + P(d7) P(YES/d7) P(NO/d7) + P(d8) P(YES/d8) P(NO/d8) +\n",
    "                 P(d9) P(YES/d9) P(NO/d9) + P(d10) P(YES/d10) P(NO/d10) + P(d11) P(YES/d11) P(NO/d11) + \n",
    "                 P(d12) P(YES/d12) P(NO/d12) + P(d13) P(YES/d13) P(NO/d13) + P(d14) P(YES/d14) P(NO/d14)\n",
    "           = 1/14 * 0/1 * 1/1 + \n",
    "             1/14 * 0/1 * 1/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 0/1 * 1/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 0/1 * 1/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 1/1 * 0/1 +\n",
    "             1/14 * 1/1 * 0/1 +\n",
    "             1/14 * 0/1 * 1/1 +\n",
    "           = 0\n",
    "  * GiniGain(Day) = Gain(PlayTennis) -  Gini(Day)\n",
    "                  = 0.4593 - 0\n",
    "                  = 0.4593 (This shows the maximum Gain in all features)\n",
    "                  \n",
    "  * If we go by purity measure, \"Day\" column is going to be best for splitting. This is not good as the day in train set is not going to repeate. The model will fail to generalize un-seen dataset.\n",
    "  * If an attribute (feature) fragments data into tiny fragments, Information Gain or GiniGain likes it (get maximum Gain). Above one is an extream example.\n",
    "  * GiniGain or InformationGain are biased towerds attributes with many values. We can solve this issue by using GainRatio.\n",
    "  \n",
    "## Information Gain Ratio (C4.5 a successor of ID3 uses an extension to information gain C4.5, a successor of ID3 uses an extension to information gain known as gain ratio)\n",
    "  * Basic idea to penalize InformationGain for choosing attributes that give lots of tiny subsets (A modification of information gain that reduces its bias towerds attributes with many values).\n",
    "  * Applies a kind of normalization to information gain using a split information value\n",
    "  * The **Split Information Value** represents the potential information generated by splitting the **training data set D** into **v partitions**, corresponding to **v outcomes on attribute A**.\n",
    "  \n",
    "   $$ SplitInfo_A (D) = -\\sum_{j=1}^v \\frac{|D_j|}{D} log_2 \\frac{|D_j|}{D} $$\n",
    "    * **High splitInfo:** partitions have more or less the same size (uniform)\n",
    "    * **Low split Info:** few partitions hold most of the tuples (peaks)   \n",
    "  * The Gain Ratio is defined as\n",
    "  $$ GainRatio(A) = \\frac{Gain(A)}{SplitInhfo(A)}$$\n",
    "  * **The attribute with the maximum gain ratio is selected as the splitting attribute**\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Overfitting and Pruning\n",
    "  * Decision Trees split data until it ends up with PURE sets.\n",
    "  * Sometimes it splits until leaves have one example. This is not good as it works with training set only and not generalizes well on an unseen dataset.\n",
    "  * With size of the tree, the tree model overfits on Training set. \n",
    "   <img height=\"800\" width=\"900\" src=\"tree_size_vs_accuracy.png\">\n",
    "   \n",
    "## How to avoid Overfitting\n",
    "  * Way 1 : Pre-Pruning - Do statistical significance (Chi_Squre test - Could this due to by chance) on each split and stop splitting if \"Not Significant\"\n",
    "  * Way 2 :  Post-Pruning - Sub-tree replacement pruning\n",
    "    * For each Node\n",
    "      * Pretend remove Node + all children from the tree\n",
    "      * Measure performance on validation set\n",
    "    * Remove Node that results in greatest improvment  \n",
    "    * Repeat until further pruning is harmful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
