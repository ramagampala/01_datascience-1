{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "  * Decision trees will have \n",
    "    * **Nodes :** Test for the value of a certain attribute\n",
    "    * **Edges :** Connects to next **\"node\" or \"leaf\"**\n",
    "    * **Leaves :** Terminal node that predicts the outcome\n",
    "    \n",
    "    <img height=\"600\" width=\"750\" src=\"./img/classification/sample_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - algorithm are know as divide & conqure algorithm\n",
    "\n",
    "### Decision Tree model is known as **non-parametric** model, not because it does not have any parameters but because the number of parameters is not determined prior to training, so the model structure is free to stick closly to data (this has a risk of overfitting). In contrast parametric models such as Linear Model has pre-determined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting).\n",
    "\n",
    "### Decision Tree models make very few assumptions about training data (unlike Linear Model, which assumes that data is linearly related to target).\n",
    "\n",
    "### Classification \n",
    "  * step 1 : split into subsets\n",
    "  * step 2 : check if the subset is pure or not (all yes or all no) ?\n",
    "  * step 3 : if all \"yes\" or \"no\" in a subset **stop**\n",
    "  * step 4 : if not **repear** step1 to 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset\n",
    "<img height=\"500\" width=\"650\" src=\"./img/classification/sample_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does a tree decide where to split ?, that is which column is best to consider as root node or further sub-nodes ? . Lets take sample data given above.\n",
    "\n",
    "  * We have different attribute selection methods (chekc purity of a subset). Based on which we are going to decide the split.\n",
    "    * For Classification Data Sets\n",
    "      * Gini Index\n",
    "      * Information Gain\n",
    "      * Gain Ratio\n",
    "         \n",
    "### Lets understand all these approaches.\n",
    "####  Gini Index (http://dni-institute.in/blogs/cart-decision-tree-gini-index-explained) :  Gini Index is one of the impurity measure. Lesser the value more significant the feature.\n",
    "\n",
    "  * GINI Index = $ \\sum_{i != j } P(i)P(j) $  = $1 - \\sum_{t=1}^k P_t^2  $ **, Maximum value of Gini Index ** could be when all target values are equally distributed.\n",
    "  * For Binary Target variable, **Maximum Gini Index** value is = $ 1 - (1/2)^2 - (1/2)^2 $ = 0.5\n",
    "  * Similarly for Nominal variable with k level, the **Maximum Gini Index** is = 1 - 1/k\n",
    "  * Minimum value of Gini Index will be 0, this happens when all observations belong to one label\n",
    "  * Gini Index for an Attribute Gini(A) = $\\sum_{v} P(v) \\sum_{i!=j} P(i/v)P(j/v) $\n",
    "\n",
    "###  ID3 algorithm produces trees with more than 2 childern, where as CART algorithm produces only binary tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets apply Gini-Index (ID3 - Algorithm) and calulate gain to decide the root-nodes for above data.\n",
    "\n",
    "  * Gain(PlayTennis) = $ 1 - P(\"YES\")^2 - P(\"NO\")^2 $\n",
    "                      = 1 - (9/14)^2 - (5/14)^2\n",
    "                      = 1 - 0.4132 - 0.1275\n",
    "                      = 0.4593\n",
    "  * Gain(Outlook) = P(sunny) P(YES/Sunny) P(NO/Sunny) + P(Overcast) P(YES/Overcast) P(NO/Overcast) + P(Rain) P(YES/Rain) P(NO/Rain)                      \n",
    "                   = 5/14 * 2/5 * 3/5 + 4/14 * 4/4 * 0/4 + 5/14 * 3/5 * 2/5   \n",
    "                   = 30/350 + 0 + 30/350\n",
    "                   = 0.1714\n",
    "  * Gain(Temperature) = P(Hot) P(YES/Hot) P(NO/Hot) + P(Mild) P(YES/Mild) P(NO/Mild) + P(Cold) P(YES/Cold) P(NO/Cold)                      \n",
    "                   = 4/14 * 2/4 * 2/4 + 6/14 * 4/6 * 2/6 + 4/14 * 3/4 * 1/4   \n",
    "                   = 16/224 + 48/504 + 12/224\n",
    "                   = 0.0714 + 0.0952 + 0.0535\n",
    "                   = 0.2201\n",
    "  * Gain(Humidity) = P(High) P(YES/High) P(NO/High) + P(Normal) P(YES/Normal) P(NO/Normal)                     \n",
    "                   = 7/14 * 3/7 * 4/7 + 7/14 * 6/7 * 1/7   \n",
    "                   = 12/98 + 6/98\n",
    "                   = 18/98\n",
    "                   = 0.1836\n",
    "  * Gain(Wind) = P(Weak) P(YES/Weak) P(NO/Weak) + P(Strong) P(YES/Strong) P(NO/Strong)                     \n",
    "                   = 7/14 * 5/7 * 2/7 + 7/14 * 3/7 * 4/7   \n",
    "                   = 10/98 + 12/98\n",
    "                   = 22/98\n",
    "                   = 0.2244        \n",
    "### Now calculate Gini Gain for each attribute.\n",
    "  * GiniGain(Outlook) = Gain(PlayTennis) - Gain(Overlook) \n",
    "                       = 0.4593 - 0.1714 \n",
    "                       = 0.2879\n",
    "  * GiniGain(Temperature) = Gain(PlayTennis) - Gain(Temperature) \n",
    "                       = 0.4593 - 0.2201\n",
    "                       = 0.2392\n",
    "  * GiniGain(Humidity) = Gain(PlayTennis) - Gain(Humidity) \n",
    "                       = 0.4593 - 0.1836\n",
    "                       = 0.2757\n",
    "  * GiniGain(Wind) = Gain(PlayTennis) - Gain(Wind) \n",
    "                       = 0.4593 - 0.2244\n",
    "                       = 0.2349                      \n",
    "### The Gini Gain for \"Outlook\" attribute is more, hence we select Outlook as root node.\n",
    "\n",
    "<img height=\"700\" width=\"900\" src=\"./img/classification/first_split.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Second level split for Outlook = \"Sunny\" (which feature to split on?)\n",
    "\n",
    "<img height=\"400\" width=\"500\" src=\"./img/classification/outlook_sunny.png\">\n",
    "\n",
    "  * Gain(PlayTennis) = $ 1 - P(\"YES\")^2 - P(\"NO\")^2 $\n",
    "                      = 1 - (2/5)^2 - (3/5)^2\n",
    "                      = 1 - 0.16 - 0.36\n",
    "                      = 0.48\n",
    "                      \n",
    "  * Gain(Temperature) = P(Hot) P(YES/Hot) P(NO/Hot) + P(Mild) P(YES/Mild) P(NO/Mild) + P(Cold) P(YES/Cold) P(NO/Cold)                      \n",
    "                   = 2/5 * 0/2 * 2/2 + 2/5 * 1/2 * 1/2 + 1/5 * 1/1 * 0/1   \n",
    "                   = 0 + 1/10 + 0\n",
    "                   = 0.1\n",
    "  * Gain(Humidity) = P(High) P(YES/High) P(NO/High) + P(Normal) P(YES/Normal) P(NO/Normal)                     \n",
    "                   = 3/5 * 0/3 * 3/3 + 2/5 * 2/2 * 0/2   \n",
    "                   = 0\n",
    "  * Gain(Wind) = P(Weak) P(YES/Weak) P(NO/Weak) + P(Strong) P(YES/Strong) P(NO/Strong)                     \n",
    "                   = 3/5 * 1/3 * 2/3 + 2/5 * 1/2 * 1/2  \n",
    "                   = 2/15 + 1/10\n",
    "                   = 0.2333\n",
    "### Gini Gain for each attribute\n",
    "  * GiniGain(Temperature) =  Gain(PlayTennis) - Gain(Temperature)\n",
    "                          = 0.48 - 0.1\n",
    "                          = 0.38\n",
    "  * GiniGain(Humidity) =  Gain(PlayTennis) - Gain(Humidity)\n",
    "                          = 0.48 - 0\n",
    "                          = 0.48          \n",
    "  * GiniGain(Wind) =  Gain(PlayTennis) - Gain(Wind)\n",
    "                          = 0.48 - 0.2333\n",
    "                          = 0.2467   \n",
    "### Gini Gain for \"Humidity\" is more, hence the split for Outlook=\"Sunny\" node is on \"Humidity\" \n",
    "<img height=\"700\" width=\"900\" src=\"./img/classification/second_1_split.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second level split for Outlook = \"Overcast\". No need to split as \"PlayTenis\" = YES for all records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second level split for Outlook = \"Rain\" (which feature to split on?)\n",
    "\n",
    "  * Gain(PlayTennis) = $ 1 - P(\"YES\")^2 - P(\"NO\")^2 $\n",
    "                      = 1 - (3/5)^2 - (2/5)^2\n",
    "                      = 1 - 0.36 - 0.16 \n",
    "                      = 0.48\n",
    "                      \n",
    "  * Gain(Temperature) = P(Mild) P(YES/Mild) P(NO/Mild) + P(Cold) P(YES/Cold) P(NO/Cold)                      \n",
    "                   = 3/5 * 2/3 * 1/3 + 2/5 * 1/2 * 1/2   \n",
    "                   = 2/15 + 1/10\n",
    "                   = 0.2333\n",
    "  * Gain(Humidity) = P(High) P(YES/High) P(NO/High) + P(Normal) P(YES/Normal) P(NO/Normal)                     \n",
    "                   = 2/5 * 1/2 * 1/2 + 3/5 * 2/3 * 1/3   \n",
    "                   = 1/10 + 2/15\n",
    "                   = 0.2333\n",
    "  * Gain(Wind) = P(Weak) P(YES/Weak) P(NO/Weak) + P(Strong) P(YES/Strong) P(NO/Strong)                     \n",
    "                   = 2/5 * 2/2 * 0/2 + 3/5 * 1/3 * 2/3  \n",
    "                   = 0 + 2/15\n",
    "                   = 0.1333\n",
    "### Gini Gain for each attribute\n",
    "  * GiniGain(Temperature) =  Gain(PlayTennis) - Gain(Temperature)\n",
    "                          = 0.48 - 0.2333\n",
    "                          = 0.2467 \n",
    "  * GiniGain(Humidity) =  Gain(PlayTennis) - Gain(Humidity)\n",
    "                          = 0.48 - 0.2333\n",
    "                          = 0.2467           \n",
    "  * GiniGain(Wind) =  Gain(PlayTennis) - Gain(Wind)\n",
    "                          = 0.48 - 0.1333\n",
    "                          = 0.3467   \n",
    "### Gini Gain for \"Wind\" is more, hence the split when overcast=\"Rain\" is on \"Wind\" \n",
    "<img height=\"700\" width=\"900\" src=\"./img/classification/second_2_split.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Node needed to be splitted is Outlook - \"Rain\" -> Wind - \"Strong\" one. As this Node has records of two classes.\n",
    "\n",
    "### Third level split for Outlook - \"Rain\" -> Wind - \"Strong\" (which feature to split on?)\n",
    "\n",
    "  * Gain(PlayTennis) = $ 1 - P(\"YES\")^2 - P(\"NO\")^2 $\n",
    "                      = 1 - (1/3)^2 - (2/3)^2\n",
    "                      = 1 - 0.1111 - 0.4444\n",
    "                      = 0.4445\n",
    "                      \n",
    "  * Gain(Temperature) = P(Mild) P(YES/Mild) P(NO/Mild) + P(Cold) P(YES/Cold) P(NO/Cold)                      \n",
    "                   = 2/3 * 1/2 * 1/2 + 1/3 * 0/2 * 1/1   \n",
    "                   = 1/6 + 0\n",
    "                   = 0.1666\n",
    "  * Gain(Humidity) = P(High) P(YES/High) P(NO/High) + P(Normal) P(YES/Normal) P(NO/Normal)                     \n",
    "                   = 1/3 * 0/1 * 1/1 + 2/3 * 1/2 * 1/2   \n",
    "                   = 0 + 1/6\n",
    "                   = 0.1666\n",
    "\n",
    "### Gini Gain for each attribute\n",
    "  * GiniGain(Temperature) =  Gain(PlayTennis) - Gain(Temperature)\n",
    "                          = 0.4445 - 0.1666\n",
    "                          = 0.2779 \n",
    "  * GiniGain(Humidity) =  Gain(PlayTennis) - Gain(Humidity)\n",
    "                          = 0.4445 - 0.1666\n",
    "                          = 0.2779        \n",
    "### Gini Gain for \"Temperature\" and \"Humidity\"  are equal, hence we can split on any.  I chose \"Temperature\"\n",
    "<img height=\"700\" width=\"900\" src=\"./img/classification/third_1_split.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth split is intutive, we can caluluate GiniGain if we want. The final Tree is \n",
    "\n",
    "<img height=\"800\" width=\"900\" src=\"./img/classification/fourth_1_split.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We Build a Tree using GiniGain, How does Tree classification work?. It works by traversing through the nodes.\n",
    "  * Let us look at below example.\n",
    "  <img height=\"100\" width=\"400\" src=\"./img/classification/tree_classification_data.png\">\n",
    "  <img height=\"800\" width=\"900\" src=\"./img/classification/tree_classification.png\">\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain \n",
    "\n",
    "  * Need to calculate Entropy H(Y) = $ - \\sum_{i=1}^K P_i log_2 P_i $ , K - number of classes in Y. This will measure \"uncertainity\" or \"impurity\" of the set.\n",
    "  * For the above dataset Entropy H(PlayTennis) = - p(\"YES\") log P(\"YES\") - P(\"NO\") log P(\"NO\")\n",
    "                                       = - 9/14 log(9/14) - 5/14 log(5/14)\n",
    "                                       = -9/14 (-0.6374) - 5/14 (-1.4854)\n",
    "                                       = 0.4098 + 0.5305\n",
    "                                       = 0.9403\n",
    "  * Condition entropy (Expected entropy of Y knowing the values of X) - <img height=\"100\" width=\"400\"  src=\"./img/classification/conditional_entropy.png\">\n",
    "  \n",
    "  * Information Gain (Reduction of uncertainty)   G(Y/X) = H(Y) - H(Y/X)\n",
    "\n",
    "  \n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * ConditionalEntropy(PlayTennis/Outlook) = \n",
    "                                    - P(\"Outlook-Sunny\") [P(\"YES\"/\"Outlook-Sunny\") log P(\"YES\"/\"Outlook-Sunny\") +   \n",
    "                                                           P(\"NO\"/\"Outlook-Sunny\") log P(\"NO\"/\"Outlook-Sunny\")] -\n",
    "                                    - P(\"Outlook-Overcast\") [P(\"YES\"/\"Outlook-Overcast\") log P(\"YES\"/\"Outlook-Overcast\") +\n",
    "                                                           P(\"NO\"/\"Outlook-Overcast\") log P(\"NO\"/\"Outlook-Overcast\")] -\n",
    "                                    - P(\"Overlook-Rain\") [P(\"YES\"/\"Outlook-Rain\") log P(\"YES\"/\"Outlook-Rain\") +\n",
    "                                                           P(\"NO\"/\"Outlook-Rain\") log P(\"NO\"/\"Outlook-Rain\")]                                                           \n",
    "                                  = - 5/14 [2/5 log 2/5 + 3/5 log 3/5]  \n",
    "                                    - 4/14 [4/4 log 4/4 + 0/4 log 0/4] \n",
    "                                    - 5/14 [3/5 log 3/5 + 2/5 log 2/5]\n",
    "                                  = - 0.3571 [0.4 * -1.3219 + 0.6 * -0.7369]\n",
    "                                    - 0.2857 [1 * 0 + 0 * unknown]\n",
    "                                    - 0.3571 [0.6 * -0.7369 + 0.4 * -1.3219]\n",
    "                                  = - 0.3571 [ - 0.5288 - 0.4421 ]\n",
    "                                    - 0.2857 [0 + 0]\n",
    "                                    - 0.3571 [- 0.4421 - 0.5288]  \n",
    "                                  = 2 * 0.3571 * 0.9709\n",
    "                                  = 0.6934\n",
    "                                  \n",
    "  * ConditionalEntropy(PlayTennis/Temperature) = \n",
    "                                    - P(\"Temperature-Hot\") [P(\"YES\"/\"Temperature-Hot\") log P(\"YES\"/\"Temperature-Hot\") +   \n",
    "                                                           P(\"NO\"/\"Temperature-Hot\") log P(\"NO\"/\"Temperature-Hot\")] -\n",
    "                                    - P(\"Temperature-Cool\") [P(\"YES\"/\"Temperature-Cool\") log P(\"YES\"/\"Temperature-Cool\") +\n",
    "                                                           P(\"NO\"/\"Temperature-Cool\") log P(\"NO\"/\"Temperature-Cool\")] -\n",
    "                                    - P(\"Temperature-Mild\") [P(\"YES\"/\"Temperature-Mild\") log P(\"YES\"/\"Temperature-Mild\") +\n",
    "                                                           P(\"NO\"/\"Temperature-Mild\") log P(\"NO\"/\"Temperature-Mild\")]                                    \n",
    "                                  = - 4/14 [2/4 log 2/4 + 2/4 log 2/4]\n",
    "                                    - 6/14 [4/6 log 4/6 + 2/6 log 2/6]\n",
    "                                    - 4/14 [3/4 log 3/4 + 1/4 log 1/4]\n",
    "                                  = - 0.2857 [-0.5 - 0.5]\n",
    "                                    - 0.4286 [0.6667 * -0.5848 + 0.3333 * -1.5851]\n",
    "                                    - 0.2857 [0.75 * -0.4150 +  0.25 * -2]\n",
    "                                  = 0.2857 +  0.4286 [0.3899 + 0.5283] + 0.2857 [0.3112 + 0.5]\n",
    "                                  = 0.2857 + 0.3935 + 0.2317\n",
    "                                  = 0.9109\n",
    "                                  \n",
    "  * Like wise we have to calculate ConditionalEntropy(PlayTennis/Humidity), ConditionalEntropy(PlayTennis/Wind).\n",
    "### Once after calculating Conditional Entropies, calculate Information Gain.\n",
    "  * InformationGain(PlayTennis/Outlook) = H(PlayTennis) - ConditionalEntropy(PlayTennis/Outlook)\n",
    "                                        = 0.9403 - 0.6934 = 0.2469\n",
    "  * InformationGain(PlayTennis/Outlook) = H(PlayTennis) - ConditionalEntropy(PlayTennis/Outlook)\n",
    "                                        = 0.9403 - 0.9109 = 0.0294\n",
    "  *  ...\n",
    "  *  ...\n",
    "  * **The feature with highest information gain going to be the splitting feature.**\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with \"Gini Gain\" (same problem will happen for \"Infomation Gain\" also).\n",
    "  * Suppose that we have taken \"Day\" attribute for spliting. This is going to generate a tree with singleton nodes, which are purest. If we go by purity measure, split on \"Day\" column is going to be best. This is not proper as any day is not going to repeate. The model will fail for unseen dataset.\n",
    "  \n",
    "  * Gini(Day) = P(d1) P(YES/d1) P(NO/d1) + P(d2) P(YES/d2) P(NO/d2) + P(d3) P(YES/d3) P(NO/d3) + P(d4) P(YES/d4) P(NO/d4) +\n",
    "                 P(d5) P(YES/d5) P(NO/d5) + P(d1) P(YES/d6) P(NO/d6) + P(d7) P(YES/d7) P(NO/d7) + P(d8) P(YES/d8) P(NO/d8) +\n",
    "                 P(d9) P(YES/d9) P(NO/d9) + P(d10) P(YES/d10) P(NO/d10) + P(d11) P(YES/d11) P(NO/d11) + \n",
    "                 P(d12) P(YES/d12) P(NO/d12) + P(d13) P(YES/d13) P(NO/d13) + P(d14) P(YES/d14) P(NO/d14)\n",
    "           = 1/14 * 0/1 * 1/1 + \n",
    "             1/14 * 0/1 * 1/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 0/1 * 1/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 0/1 * 1/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 1/1 * 0/1 + \n",
    "             1/14 * 1/1 * 0/1 +\n",
    "             1/14 * 1/1 * 0/1 +\n",
    "             1/14 * 0/1 * 1/1 +\n",
    "           = 0\n",
    "  * GiniGain(Day) = Gain(PlayTennis) -  Gini(Day)\n",
    "                  = 0.4593 - 0\n",
    "                  = 0.4593 (This shows the maximum Gain in all features)\n",
    "                  \n",
    "  * If we go by purity measure, \"Day\" column is going to be best for splitting. This is not good as the day in train set is not going to repeate. The model will fail to generalize un-seen dataset.\n",
    "  * If an attribute (feature) fragments data into tiny fragments, Information Gain or GiniGain likes it (get maximum Gain). Above one is an extream example.\n",
    "  * GiniGain or InformationGain are biased towerds attributes with many values. We can solve this issue by using GainRatio.\n",
    "  \n",
    "## Information Gain Ratio (C4.5 a successor of ID3 uses an extension to information gain C4.5, a successor of ID3 uses an extension to information gain known as gain ratio)\n",
    "  * Basic idea to penalize InformationGain for choosing attributes that give lots of tiny subsets (A modification of information gain that reduces its bias towerds attributes with many values).\n",
    "  * Applies a kind of normalization to information gain using a split information value\n",
    "  * The **Split Information Value** represents the potential information generated by splitting the **training data set D** into **v partitions**, corresponding to **v outcomes on attribute A**.\n",
    "  \n",
    "   $$ SplitInfo_A (D) = -\\sum_{j=1}^v \\frac{|D_j|}{D} log_2 \\frac{|D_j|}{D} $$\n",
    "    * **High splitInfo:** partitions have more or less the same size (uniform)\n",
    "    * **Low split Info:** few partitions hold most of the tuples (peaks)   \n",
    "  * The Gain Ratio is defined as\n",
    "  $$ GainRatio(A) = \\frac{Gain(A)}{SplitInhfo(A)}$$\n",
    "  * **The attribute with the maximum gain ratio is selected as the splitting attribute**\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Attribute Selection Measures\n",
    "  * The three measures, in general, return good results but\n",
    "    * Gini Index\n",
    "      * Biased towards multivalued attributes\n",
    "      * Has difficulties when the number of classes is large\n",
    "      * Tends to favor tests that result in equal-sized partitions and purity in both partitions  \n",
    "    * Information Gain\n",
    "      * Biased towards multivalued attributes\n",
    "    * Gain Ratio\n",
    "      * Tends to prefer unbalanced splits in which one partition is much smaller than the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Overfitting and Pruning\n",
    "  * Decision Trees split data until it ends up with PURE sets.\n",
    "  * Sometimes it splits until leaves have one example. This is not good as it works with training set only and not generalizes well on an unseen dataset.\n",
    "  * With size of the tree, the tree model overfits on Training set. \n",
    "   <img height=\"800\" width=\"900\" src=\"./img/classification/tree_size_vs_accuracy.png\">\n",
    "   \n",
    "## How to avoid Overfitting\n",
    "  * Way 1 : Pre-Pruning - Do statistical significance (Chi_Squre test - Could this due to by chance) on each split and stop splitting if \"Not Significant\"\n",
    "  * Way 2 :  Post-Pruning - Sub-tree replacement pruning\n",
    "    * For each Node\n",
    "      * Pretend remove Node + all children from the tree\n",
    "      * Measure performance on validation set\n",
    "    * Remove Node that results in greatest improvment  \n",
    "    * Repeat until further pruning is harmful\n",
    "    \n",
    "## Overfitting & Tree Pruning (This can be applied to Classification or Regression)\n",
    "  * **Overfitting :** The process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance. This is because the resulting tree might be too complex. A smaller tree with fewer splits (that is, fewer regions $ R_1, R_2, . . . , R_J $) might lead to lower variance and better interpretation at the cost of a little bias.\n",
    "  * **Strategy to Fix Overfitting :** Therefore, a better strategy is to grow a very large tree T , and then prune it back in order to obtain a subtree.\n",
    "  * Given a subtree, we can estimate its test error using cross-validation or the validation set approach. However, estimating the cross-validation error for every possible subtree would be too cumbersome, since there is an extremely large number of possible subtrees. Instead, we need a way to select a small set of subtrees for consideration.\n",
    "  * **Cost complexity pruning ** — also known as **weakest link pruning—gives** us a way to do just this. The overfitting is directly proportion to the **depth** or **number of leaf nodes** of a tree. Cost Complexity Pruning focusses on number of leaf nodes, as for some trees depth may be same but tree may be complex.\n",
    "  \n",
    "<img height=\"500\" width=\"700\" src=\"./img/classification/depth_vs_leaf_nodes.png\">  \n",
    "  \n",
    "  * <font size=5>Total Cost C(T) = Error(T) + $ \\alpha $ L(T), $ \\alpha $ - tuning parameter </font>\n",
    "  <br>  \n",
    "    * <font size=3> -> L(T) - function measures number of leaf of a tree</font>\n",
    "  <br>\n",
    "    * <font size=4> -> if $ \\alpha $ = 0, this is standarded learning tree</font>\n",
    "<img height=\"400\" width=\"600\" src=\"./img/classification/tree_full_depth.png\">        \n",
    "    * <font size=4> -> if $ \\alpha = infinite $, then only root node with lot of prediction error</font>\n",
    "<img height=\"200\" width=\"180\" src=\"./img/classification/only_root.png\">          \n",
    "    * <font size=3> -> ** Need to find an $ \\alpha $ in between 0 and infinite. We can select a value of $ \\alpha $ using a validation set or using cross-validation.**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Vs Linear Models\n",
    "  * **A linear boundary (left) will outperform a decision tree that performs splits parallel to the axes (right)**\n",
    "<img height=\"400\" width=\"600\" src=\"./img/classification/linear_boundary.png\">    \n",
    "  * **Here a linear model is unable to capture the true decision boundary (left), whereas a decision tree is successful (right).**\n",
    "<img height=\"400\" width=\"600\" src=\"./img/classification/non_linear_boundary.png\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summay of Decision Trees\n",
    "  ### * Decision Trees have relatively faster learning speed than other methods\n",
    "  ### * Easy to understand classification rules\n",
    "  ### * Information Gain, Ratio Gain and Gini Index are the most common methods of attribute selection\n",
    "  ### * Tree pruning is necessary to remove unreliable branches\n",
    "  ### * No formal distributional assumptions\n",
    "  ### * Can automatically fit highly non-linear interactions\n",
    "  ### * Automatic variable selection\n",
    "  ### * Handle missing values\n",
    "  ### <font color=\"red\">* Instability – if we change the data a little, the tree picture can change a lot </font>\n",
    "  ### <font color=\"red\">* Scalability is an issue for large datasets</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
